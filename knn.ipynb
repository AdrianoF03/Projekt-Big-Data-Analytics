{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd #Bibliothek für Datenmanipulation und -analyse.\n",
    "import numpy as np #Bibliothek für mathematische Funktionen.\n",
    "import matplotlib.pyplot as plt #Bibliothek zum Erstellen von statischen, animierten und interaktiven Visualisierungen.\n",
    "import seaborn as sns #Bibliothek für statistische Datenvisualisierung basierend auf matplotlib.\n",
    "# knn\n",
    "from sklearn.neighbors import KNeighborsClassifier #K-Nearest Neighbors Klassifikator aus scikit-learn.\n",
    "from sklearn import neighbors #Modul für K-Nearest Neighbors Algorithmen.\n",
    "from sklearn.model_selection import cross_val_score #Funktion zur Bewertung eines Scores durch Kreuzvalidierung.\n",
    "from sklearn.preprocessing import StandardScaler #Standardisiert Merkmale, indem der Mittelwert entfernt und auf die Einheitsskala skaliert wird.\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV #Funktion zum Aufteilen von Arrays oder Matrizen in zufällige Trainings- und Testsätze.\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay #Modul zur Berechnung von Klassifikationsmetriken.\n",
    "from mlxtend.plotting import plot_decision_regions #Funktion zum Plotten von Entscheidungsregionen von Klassifikatoren.\n",
    "# Entscheidungsbaum\n",
    "from sklearn.tree import DecisionTreeClassifier #Entscheidungsbaum-Klassifikator aus scikit-learn.\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier #Random Forest Klassifikator aus scikit-learn.\n",
    "#Neurnales Netz\n",
    "from tensorflow.keras.models import Sequential #Sequenzielles Modell aus Keras, einer High-Level-API für neuronale Netze.\n",
    "from tensorflow.keras.utils import to_categorical #Funktion zur Umwandlung eines Klassenvektors (Ganzzahlen) in eine binäre Klassenmatrix.\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization #Dichte (vollständig verbundene) Schicht aus Keras.\n",
    "from tensorflow.keras.callbacks import EarlyStopping #Callback zum Stoppen des Trainings, wenn eine überwachte Metrik sich nicht mehr verbessert.\n",
    "\n",
    "\n",
    "# CSV-Datei einlesen\n",
    "telefonkunden = pd.read_csv('telefonkunden.csv')\n",
    "\n",
    "# Wie sieht der Datensatz aus?\n",
    "print(telefonkunden.head)\n",
    "telefonkunden.shape\n",
    "\n",
    "# - 2. den Datensatz in Trainings- und Lerndatensatz aufteilen\n",
    "\n",
    "# Dafür erstellen wir zunächst (wie auch bei den Weinen) zwei separate Datensätze mit den Input- (X) und der Outputvariablen (Y).\n",
    "X = telefonkunden.drop(columns=['custcat'])\n",
    "Y = telefonkunden.custcat\n",
    "X.head()\n",
    "Y.head()\n",
    "\n",
    "# Jetzt können wir wieder mit train_test_split aus scikit-learn den Datensatz in Trainings- und Testdaten aufteilen\n",
    "\n",
    "X_training, X_test, Y_training, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1, stratify = Y)\n",
    "# Wie vorher: Wir teilen in 20% Lern- und 80% Testdaten auf (test_size), lassen die Aufteilung zur Reproduzierbarkeit der Ergebnisse immer gleich aufteilen (random_state) und sorgen dafür, dass die Klassen gleichmäßig aufgeteilt sind (stratify).\n",
    "\n",
    "# Nun wird das Modelll trainiert. Wir verwenden zunächst k=5 Nachbarn.\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "vorhersage_scikit = knn.fit(X_training, Y_training)\n",
    "\n",
    "# - 3. eine Vorhersage mit dem kNN-Algorithmus treffen\n",
    "vorhersage_scikit=knn.predict(X_test)\n",
    "#Ergebnis \n",
    "knn.score(X_test, Y_test)\n",
    "# Die bisherige Vorgehensweise hieß Holdout-Methode: Man reserviert eine Menge als Testdaten, den Rest als Trainingsdaten. Allerdings kann dies zu Probleme führen: \n",
    "# Vielleicht wurden die Daten unglücklich aufgeteilt?Zur Lösung bedient man sich der sog. k-fache Kreuzvalidierung, wie auch in der Vorlesung besprochen: \n",
    "# Bei der Kreuzvalidierung wird der Datensatz zufällig in Gruppen aufgeteilt. Eine der Gruppen wird als Testsatz und der Rest als Trainingssatz verwendet. \n",
    "# Das Modell wird mit dem Trainingssatz trainiert und mit dem Testsatz bewertet. Dann wird der Prozess wiederholt, bis jede einzelne Gruppe als Testsatz verwendet wurde. \n",
    "# Bei der 5-fachen Kreuzvalidierung werden also die Daten in 5 Gruppen aufgeteilt und 5-mal angepasst und bewertet, wobei jedes Mal der Genauigkeitswert in einem Array abgespeichert wird. So hat jede der 5 Gruppen eine Chance, als Testdatensatz zu dienen.\n",
    "# Neuen Klassifikator verwenden, mit unserem idealen k=5\n",
    "klassifikator_kreuzvalidierung = KNeighborsClassifier(n_neighbors=13)\n",
    "\n",
    "# Genauigkeiten mit einem trainierten Modell mit 5 Gruppen\n",
    "kreuzvalidierung_genauigkeiten = cross_val_score(klassifikator_kreuzvalidierung, X, Y, cv = 5)\n",
    "\n",
    "# Genauigkeiten ausgeben\n",
    "print(kreuzvalidierung_genauigkeiten)\n",
    "# Nehmen wir einfach mal den Mittelwert dieser Genauigkeiten, und sehen, dass wir die Genauigkeit nochmals erhöhen konnten von ca. 66% auf ca. 71%!\n",
    "print(np.mean(kreuzvalidierung_genauigkeiten))\n",
    "# Wir konnten also eine Erhöhung der Genauigkeit erreichen!\n",
    "# Definieren wir zunächst einen neuen Klassifikator:\n",
    "knn2 = KNeighborsClassifier()\n",
    "\n",
    "# Nun erstellen wir das Dictionary mit den k-Werten für die Hyperparameter-Optimimerung, die uns interessieren:\n",
    "k_grid = {'n_neighbors': np.arange(1, 50)}\n",
    "\n",
    "# Nun lassen wir die Grid-Suche für jedes k durchlaufen, jeweils mit einer 5-fachen Kreuzvalidierung:\n",
    "knn_grid = GridSearchCV(knn2, k_grid, cv = 5)\n",
    "\n",
    "# Nun trainieren wir das Modell mit den Daten und den darüber definierten Parametern:\n",
    "knn_grid.fit(X, Y)\n",
    "\n",
    "# Was ist nun die beste Anzahl an Nachbarn?\n",
    "print(knn_grid.best_params_)\n",
    "\n",
    "# Der Genauigkeits-Score bei dieser Anzahl an Nachbarn ist nochmal höher\n",
    "knn_grid.best_score_\n",
    "# Man kann die Funktion auch alle Genauigkeits-Scores ausgeben lassen. Dies sind also für jedes k die durchschnittlichen Werte der Genauigkeits-Scores aus je allen 5 Durchläufen der Kreuzvalidierung!\n",
    "scores=knn_grid.cv_results_['mean_test_score']\n",
    "print(scores)\n",
    "\n",
    "# Wir definieren eine Bildgröße\n",
    "plt.figure(figsize=(10,6))\n",
    "# Wir plotten auf der x-Achse von 1 bis 49 (also die k's die wir oben in der for-Schleife durchprobiert haben)\n",
    "plt.plot(range(1, len(scores) + 1), scores)\n",
    "# Wir beschriften noch die Achsen\n",
    "plt.title('Genauigkeit vs. k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Genauigkeit')\n",
    "\n",
    "# Wahrheitsmatrix für das KNN-Modell\n",
    "conf_matrix = confusion_matrix(Y_test, vorhersage_scikit)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=knn.classes_, yticklabels=knn.classes_)\n",
    "plt.xlabel('Vorhergesagte Labels')\n",
    "plt.ylabel('Wahre Labels')\n",
    "plt.title('Wahrheitsmatrix für KNN-Modell')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
