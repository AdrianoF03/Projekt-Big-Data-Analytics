{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd #Bibliothek für Datenmanipulation und -analyse.\n",
    "import numpy as np #Bibliothek für mathematische Funktionen.\n",
    "import matplotlib.pyplot as plt #Bibliothek zum Erstellen von statischen, animierten und interaktiven Visualisierungen.\n",
    "import seaborn as sns #Bibliothek für statistische Datenvisualisierung basierend auf matplotlib.\n",
    "# knn\n",
    "from sklearn.neighbors import KNeighborsClassifier #K-Nearest Neighbors Klassifikator aus scikit-learn.\n",
    "from sklearn import neighbors #Modul für K-Nearest Neighbors Algorithmen.\n",
    "from sklearn.model_selection import cross_val_score #Funktion zur Bewertung eines Scores durch Kreuzvalidierung.\n",
    "from sklearn.preprocessing import StandardScaler #Standardisiert Merkmale, indem der Mittelwert entfernt und auf die Einheitsskala skaliert wird.\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV #Funktion zum Aufteilen von Arrays oder Matrizen in zufällige Trainings- und Testsätze.\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay #Modul zur Berechnung von Klassifikationsmetriken.\n",
    "from mlxtend.plotting import plot_decision_regions #Funktion zum Plotten von Entscheidungsregionen von Klassifikatoren.\n",
    "# Entscheidungsbaum\n",
    "from sklearn.tree import DecisionTreeClassifier #Entscheidungsbaum-Klassifikator aus scikit-learn.\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier #Random Forest Klassifikator aus scikit-learn.\n",
    "#Neurnales Netz\n",
    "from tensorflow.keras.models import Sequential #Sequenzielles Modell aus Keras, einer High-Level-API für neuronale Netze.\n",
    "from tensorflow.keras.utils import to_categorical #Funktion zur Umwandlung eines Klassenvektors (Ganzzahlen) in eine binäre Klassenmatrix.\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization #Dichte (vollständig verbundene) Schicht aus Keras.\n",
    "from tensorflow.keras.callbacks import EarlyStopping #Callback zum Stoppen des Trainings, wenn eine überwachte Metrik sich nicht mehr verbessert.\n",
    "\n",
    "\n",
    "# CSV-Datei einlesen\n",
    "telefonkunden = pd.read_csv('telefonkunden.csv')\n",
    "\n",
    "# Wie sieht der Datensatz aus?\n",
    "print(telefonkunden.head)\n",
    "telefonkunden.shape\n",
    "\n",
    "#1. KNN - Ursprüngliche Daten - Alle Variablen\n",
    "\n",
    "# - 2. den Datensatz in Trainings- und Lerndatensatz aufteilen\n",
    "\n",
    "# Dafür erstellen wir zunächst (wie auch bei den Weinen) zwei separate Datensätze mit den Input- (X) und der Outputvariablen (Y).\n",
    "X = telefonkunden.drop(columns=['custcat'])\n",
    "Y = telefonkunden.custcat\n",
    "X.head()\n",
    "Y.head()\n",
    "\n",
    "# Jetzt können wir wieder mit train_test_split aus scikit-learn den Datensatz in Trainings- und Testdaten aufteilen\n",
    "\n",
    "X_training, X_test, Y_training, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1, stratify = Y)\n",
    "# Wie vorher: Wir teilen in 20% Lern- und 80% Testdaten auf (test_size), lassen die Aufteilung zur Reproduzierbarkeit der Ergebnisse immer gleich aufteilen (random_state) und sorgen dafür, dass die Klassen gleichmäßig aufgeteilt sind (stratify).\n",
    "\n",
    "# Nun wird das Modelll trainiert. Wir verwenden zunächst k=5 Nachbarn.\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "vorhersage_scikit = knn.fit(X_training, Y_training)\n",
    "\n",
    "# - 3. eine Vorhersage mit dem kNN-Algorithmus treffen\n",
    "vorhersage_scikit=knn.predict(X_test)\n",
    "#Ergebnis \n",
    "knn.score(X_test, Y_test)\n",
    "# Die bisherige Vorgehensweise hieß Holdout-Methode: Man reserviert eine Menge als Testdaten, den Rest als Trainingsdaten. Allerdings kann dies zu Probleme führen: \n",
    "# Vielleicht wurden die Daten unglücklich aufgeteilt?Zur Lösung bedient man sich der sog. k-fache Kreuzvalidierung, wie auch in der Vorlesung besprochen: \n",
    "# Bei der Kreuzvalidierung wird der Datensatz zufällig in Gruppen aufgeteilt. Eine der Gruppen wird als Testsatz und der Rest als Trainingssatz verwendet. \n",
    "# Das Modell wird mit dem Trainingssatz trainiert und mit dem Testsatz bewertet. Dann wird der Prozess wiederholt, bis jede einzelne Gruppe als Testsatz verwendet wurde. \n",
    "# Bei der 5-fachen Kreuzvalidierung werden also die Daten in 5 Gruppen aufgeteilt und 5-mal angepasst und bewertet, wobei jedes Mal der Genauigkeitswert in einem Array abgespeichert wird. So hat jede der 5 Gruppen eine Chance, als Testdatensatz zu dienen.\n",
    "# Neuen Klassifikator verwenden, mit unserem idealen k=5\n",
    "klassifikator_kreuzvalidierung = KNeighborsClassifier(n_neighbors=13)\n",
    "\n",
    "# Genauigkeiten mit einem trainierten Modell mit 5 Gruppen\n",
    "kreuzvalidierung_genauigkeiten = cross_val_score(klassifikator_kreuzvalidierung, X, Y, cv = 5)\n",
    "\n",
    "# Genauigkeiten ausgeben\n",
    "print(kreuzvalidierung_genauigkeiten)\n",
    "# Nehmen wir einfach mal den Mittelwert dieser Genauigkeiten, und sehen, dass wir die Genauigkeit nochmals erhöhen konnten von ca. 66% auf ca. 71%!\n",
    "print(np.mean(kreuzvalidierung_genauigkeiten))\n",
    "# Wir konnten also eine Erhöhung der Genauigkeit erreichen!\n",
    "# Definieren wir zunächst einen neuen Klassifikator:\n",
    "knn2 = KNeighborsClassifier()\n",
    "\n",
    "# Nun erstellen wir das Dictionary mit den k-Werten für die Hyperparameter-Optimimerung, die uns interessieren:\n",
    "k_grid = {'n_neighbors': np.arange(1, 50)}\n",
    "\n",
    "# Nun lassen wir die Grid-Suche für jedes k durchlaufen, jeweils mit einer 5-fachen Kreuzvalidierung:\n",
    "knn_grid = GridSearchCV(knn2, k_grid, cv = 5)\n",
    "\n",
    "# Nun trainieren wir das Modell mit den Daten und den darüber definierten Parametern:\n",
    "knn_grid.fit(X, Y)\n",
    "\n",
    "# Was ist nun die beste Anzahl an Nachbarn?\n",
    "print(knn_grid.best_params_)\n",
    "\n",
    "# Der Genauigkeits-Score bei dieser Anzahl an Nachbarn ist nochmal höher\n",
    "knn_grid.best_score_\n",
    "# Man kann die Funktion auch alle Genauigkeits-Scores ausgeben lassen. Dies sind also für jedes k die durchschnittlichen Werte der Genauigkeits-Scores aus je allen 5 Durchläufen der Kreuzvalidierung!\n",
    "scores=knn_grid.cv_results_['mean_test_score']\n",
    "print(scores)\n",
    "\n",
    "# Wir definieren eine Bildgröße\n",
    "plt.figure(figsize=(10,6))\n",
    "# Wir plotten auf der x-Achse von 1 bis 49 (also die k's die wir oben in der for-Schleife durchprobiert haben)\n",
    "plt.plot(range(1, len(scores) + 1), scores)\n",
    "# Wir beschriften noch die Achsen\n",
    "plt.title('Genauigkeit vs. k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Genauigkeit')\n",
    "\n",
    "# Wahrheitsmatrix für das KNN-Modell\n",
    "conf_matrix = confusion_matrix(Y_test, vorhersage_scikit)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=knn.classes_, yticklabels=knn.classes_)\n",
    "plt.xlabel('Vorhergesagte Labels')\n",
    "plt.ylabel('Wahre Labels')\n",
    "plt.title('Wahrheitsmatrix für KNN-Modell')\n",
    "plt.show()\n",
    "\n",
    "# KNN Adriano - optimiert\n",
    "# Merkmale und Zielvariable definieren\n",
    "X = telefonkunden[['tenure','age','income','address','employ','ed']] # Merkmale, die für die Vorhersage von 'custcat' relevant sind\n",
    "Y = telefonkunden['custcat']\n",
    "\n",
    "# Daten in Trainings- und Testset aufteilen 80/20 Split|Stratified --> gleiche Verteilung der Zielvariable\n",
    "X_training, X_test, Y_training, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1, stratify=Y)\n",
    "\n",
    "# KNN-Modell erstellen und trainieren\n",
    "knn = KNeighborsClassifier(n_neighbors=19)  # Anzahl der Nachbarn \n",
    "vorhersage_scikit = knn.fit(X_training, Y_training)\n",
    "\n",
    "vorhersage_scikit = knn.predict(X_test)\n",
    "\n",
    "# Genauigkeit der Vorhersage\n",
    "print(accuracy_score(Y_test, vorhersage_scikit))\n",
    "\n",
    "# Berechnung der Genauigkeit für verschiedene Werte von n_neighbors\n",
    "scores = []\n",
    "for k in range(1, 100):\n",
    "\tknn = KNeighborsClassifier(n_neighbors=k)\n",
    "\tknn.fit(X_training, Y_training)\n",
    "\ty_pred = knn.predict(X_test)\n",
    "\tscores.append(accuracy_score(Y_test, y_pred))\n",
    "\n",
    "# Berechnung der Genauigkeit für verschiedene Werte von n_neighbors\n",
    "scores = []\n",
    "for k in range(1, 100):\n",
    "\tknn = KNeighborsClassifier(n_neighbors=k)\n",
    "\tknn.fit(X_training, Y_training)\n",
    "\ty_pred = knn.predict(X_test)\n",
    "\tscores.append(accuracy_score(Y_test, y_pred))\n",
    "\n",
    "# Ausgabe der Genauigkeits-Scores\n",
    "print(scores)\n",
    "\n",
    "# Dies kann man auch noch grafisch darstellen. Dafür brauchen wir die matplotlib als gute Bibliothek für Datenvisualisierung:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Wir definieren eine Bildgröße\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Wir plotten auf der x-Achse von 1 bis 100 (also die k's die wir oben in der for-Schleife durchprobiert haben)\n",
    "plt.plot(range(1, 100), scores)\n",
    "# Wir beschriften noch die Achsen\n",
    "plt.title('Genauigkeit vs. k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Genauigkeit')\n",
    "plt.show()\n",
    "\n",
    "# Wahrheits Matrix\n",
    "conf_matrix = confusion_matrix(Y_test, vorhersage_scikit)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=knn.classes_, yticklabels=knn.classes_)\n",
    "plt.xlabel('Vorhergesagte Labels')\n",
    "plt.ylabel('Wahre Labels')\n",
    "plt.title('Wahrheitsmatrix Matrix für KNN-Modell')\n",
    "plt.show()\n",
    "\n",
    "# Tuning\n",
    "\n",
    "# 1. Datenaufteilung in Features (X) und Zielvariable (y)\n",
    "X = telefonkunden[['tenure','age','income','ed', 'gender', 'address']]\n",
    "Y = telefonkunden['custcat']\n",
    "\n",
    "# 2. Daten in Trainings- und Testset aufteilen (80/20)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "# 3. Feature-Skalierung (Standardisierung)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 4. Hyperparameter-Raster für k-NN definieren\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],    # Verschiedene Werte für die Anzahl der Nachbarn\n",
    "    'weights': ['uniform', 'distance'],# Gewichtung: alle Nachbarn gleich oder nach Distanz\n",
    "    'p': [1, 2]                        # Metrik (1=Manhattan, 2=Euklid)\n",
    "}\n",
    "\n",
    "# 5. GridSearchCV-Objekt mit k-NN als Schätzer instanziieren\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                      # 5-fache Kreuzvalidierung\n",
    "    scoring='accuracy'         # Gütemaß: Genauigkeit\n",
    ")\n",
    "\n",
    "# 6. GridSearch durch Training mit den verschiedenen Hyperparametern ausführen\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# 7. Beste Kombination der Hyperparameter und deren Genauigkeit ausgeben\n",
    "print(\"Beste Hyperparameter:\", grid_search.best_params_)\n",
    "print(\"Beste Genauigkeit (CV):\", grid_search.best_score_)\n",
    "\n",
    "# 8. Vorhersage mit dem besten Modell\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# 9. Modellbewertung\n",
    "print(\"k-NN Klassifikationsbericht (bestes Modell):\")\n",
    "print(classification_report(Y_test, y_pred))\n",
    "print(\"k-NN Genauigkeit (Testset):\", accuracy_score(Y_test, y_pred))\n",
    "##################### KNN Visualisierung ################\n",
    "from sklearn import neighbors\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Convert DataFrame to NumPy array\n",
    "X_np = X.values\n",
    "\n",
    "# Reduce the dimensionality of the dataset to 2 features using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_np)\n",
    "\n",
    "# Trainiere und visualisiere KNN mit verschiedenen k-Werten\n",
    "for k in [19]:\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_pca, Y.values)\n",
    "\n",
    "    plot_decision_regions(X_pca, Y.values, clf=knn, legend=2, scatter_kwargs={\"edgecolor\": \"none\"})\n",
    "    \n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.title(f\"KNN mit K={k}\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
